经常使用的激活函数主要包括以下几种：

# sgn符号函数

函数定义：

 $y=f(x)=\begin{cases}x>0,\quad y=1\\x\le0,\quad y=-1 \end{cases}​$

函数曲线：

![](https://picbed-wolf.oss-cn-shenzhen.aliyuncs.com/pic_md/20190403201511.png)

函数特点：

1.阶跃函数，非线性变化

# sigmod函数

函数定义：

 $S(x)=\frac{1}{1+e^{-x}}$

函数曲线：

![](https://picbed-wolf.oss-cn-shenzhen.aliyuncs.com/pic_md/20190403201919.png)

函数特点：

1.非线性函数，值域在（0,1）之内；

2.函数趋近于0和1时梯度消失，权重更新缓慢；

3.输出不以0为中心；

4.计算成本高昂，性能不好；

# Tanh函数

函数定义：

 $tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}$

函数曲线：

![](https://picbed-wolf.oss-cn-shenzhen.aliyuncs.com/pic_md/20190403202102.png)

函数特点：

1.非线性函数，值域在（-1,1）之内；

2.函数趋近于-1和1时同样梯度消失，权重更新缓慢；

3.修正了sigmod函数的值域中心，以0为中心；

# ReLU函数

函数定义：

 $f(x)=\begin{cases}x,x>0\\0,x\le0 \end{cases}$

函数曲线：

![](https://picbed-wolf.oss-cn-shenzhen.aliyuncs.com/pic_md/20190403202519.png)

函数特点：

1.收敛快，不会存在梯度消失的问题；

2.计算效率高，性能好；

3.不是以0为中心；

4.当x<0时，神经元处于非激活状态，后向传导时权重无法更新；

## Leaky ReLU（ReLU变体）

函数定义：

 $f(x)=\begin{cases}x,x>0\\\frac{x}{a},x\le0 \end{cases}$，其中a为$(1,+\infty)$中的一个常数值

函数曲线：

![](https://picbed-wolf.oss-cn-shenzhen.aliyuncs.com/pic_md/20190403202853.png)

函数特点：

1.收敛快，不会存在梯度消失的问题；

2.计算效率高，性能好；

3.不是以0为中心；

4.当x<0时，可以得到0.1的正梯度，防止了dead ReLU问题；

## Parametric ReLU(Leaky ReLU扩展）

函数定义：

 $f(x)=max(\frac{x}{a},x)$，其中a为$(1,+\infty)​$中的一个常数值

函数曲线：

![](https://picbed-wolf.oss-cn-shenzhen.aliyuncs.com/pic_md/20190403202853.png)

函数特点：

1.收敛快，不会存在梯度消失的问题；

2.计算效率高，性能好；

3.不是以0为中心；

4.当x<0时，可以得到0.1的正梯度，防止了dead ReLU问题；