经常使用的激活函数主要包括以下几种：

# sgn符号函数

函数定义：

 $y=f(x)=\begin{cases}x>0,\quad y=1\\x\le0,\quad y=-1 \end{cases}​$

函数曲线：

![](https://picbed-wolf.oss-cn-shenzhen.aliyuncs.com/pic_md/20190403201511.png)

函数特点：

1.阶跃函数，非线性变化

# sigmod函数

函数定义：

 $S(x)=\frac{1}{1+e^{-x}}​$

函数曲线：

![](https://picbed-wolf.oss-cn-shenzhen.aliyuncs.com/pic_md/20190403201919.png)

函数特点：

1.非线性函数，值域在（0,1）之内；

2.函数趋近于0和1时梯度消失，权重更新缓慢；

3.输出不以0为中心；

4.计算成本高昂，性能不好；

# Tanh函数

函数定义：

 $tanh(x)=\frac{1-e^{-2x}}{1+e^{-2x}}​$

函数曲线：

![](https://picbed-wolf.oss-cn-shenzhen.aliyuncs.com/pic_md/20190403202102.png)

函数特点：

1.非线性函数，值域在（-1,1）之内；

2.函数趋近于-1和1时同样梯度消失，权重更新缓慢；

3.修正了sigmod函数的值域中心，以0为中心；

# ReLU函数

函数定义：

 $f(x)=\begin{cases}x,x>0\\0,x\le0 \end{cases}$

函数曲线：

![](https://picbed-wolf.oss-cn-shenzhen.aliyuncs.com/pic_md/20190403202519.png)

函数特点：

1.收敛快，不会存在梯度消失的问题；

2.计算效率高，性能好；

3.不是以0为中心；

4.当x<0时，神经元处于非激活状态，后向传导时权重无法更新；

## Leaky ReLU（ReLU变体）

函数定义：

 $f(x)=\begin{cases}x,x>0\\\frac{x}{a},x\le0 \end{cases}$，其中a为$(1,+\infty)$中的一个常数值

函数曲线：

![](https://picbed-wolf.oss-cn-shenzhen.aliyuncs.com/pic_md/20190403202853.png)

函数特点：

1.收敛快，不会存在梯度消失的问题；

2.计算效率高，性能好；

3.不是以0为中心；

4.当x<0时，可以得到0.1的正梯度，防止了dead ReLU问题；

## Parametric ReLU(参数ReLU）

函数定义：

 $f(x)=max(\frac{x}{a_i},x)​$，其中ai为参数，当ai=0时PReLU退化为ReLU，当ai为一个较大数如100时，PReLU退化为LReLU

函数曲线：

![](https://picbed-wolf.oss-cn-shenzhen.aliyuncs.com/pic_md/20190406165031.png)

函数特点：

PReLU中的ai是根据数据变化的；



除此以外还有RReLU，该函数的aji是一个在给定的范围内随机抽取的值，这个值在测试环节就会固定下来

# Softmax函数

该函数比较特殊，它主要用在输出层，并且它的输出不仅受当前神经元输入的影响，而且受所有输出层神经元输入的影响，具体可参见函数定义公式：

 $f(z)_j=\frac{e^{z_j}}{\sum_{k=1}^{K}e^{z_k}}​$

图说明：

![](https://picbed-wolf.oss-cn-shenzhen.aliyuncs.com/pic_md/20190407112451.png)

Softmax激活函数用在输出层（Y），输出层有几个值就代表多分类的几种类别，该函数同其他激活函数（如sigmod）不同，它的输入是一个向量，向量的维度即为输出层神经元的输入（输出层多少个神经元即对应该函数输入向量多少个维度），输出也是一个向量，其维度为输出层神经元的输出，以上图为例我们对其正向传导公式进行展开：
$$
Out_{y_1}=\frac{e^{net_{y_1}}}{e^{net_{y_1}}+e^{net_{y_2}}}\\
Out_{y_2}=\frac{e^{net_{y_2}}}{e^{net_{y_1}}+e^{net_{y_2}}}
$$
那么对于样本1的输入，预测输出就是一个向量表示[Outy1,Outy2]，向量的各维度和为1，那么预测输出实际上表示了一个概率向量，而这个向量中哪个维度最高，就表示多分类结果偏向于哪个分类的概率大。上述公式可以看出，该输出层的每个神经元输出同其他激活函数不同，这个输出实际上同该层所有神经元的输入都有关。

对于Softmax来说，其损失函数一般用交叉熵而不是均方差，交叉熵函数定义如下：

$L=\sum_{j=1}^K(y_jlogy_j'+(1-y_j)log(1-y_j'))$

